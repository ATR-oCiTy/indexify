mod openai;
mod sentence_transformers;

use super::server_config::{
    self, EmbeddingModelKind::AllMiniLmL12V2, EmbeddingModelKind::AllMiniLmL6V2,
    EmbeddingModelKind::OpenAIAda02, EmbeddingModelKind::T5Base,
};
use async_trait::async_trait;
use std::collections::HashMap;
use std::sync::Arc;
use thiserror::Error;
use tracing::info;

use openai::OpenAI;
use sentence_transformers::SentenceTransformerModels;

/// An enumeration of possible errors that can occur while generating text embeddings.
#[derive(Error, Debug)]
pub enum EmbeddingGeneratorError {
    /// An error that occurs when the requested model is not found.
    #[error("model `{0}` not found")]
    ModelNotFound(String),

    /// An error that occurs when generating embeddings using a model.
    #[error("model inference error: `{0}`")]
    ModelError(String),

    /// An error that occurs when loading a model.
    #[error("model loading error: `{0}`")]
    ModelLoadingError(String),

    /// An internal error that occurs during the operation.
    #[error("internal error: `{0}`")]
    InternalError(String),

    /// An error that occurs when the required configuration is missing for a model.
    #[error("configuration `{0}`, missing for model `{1}`")]
    ConfigurationError(String, String),
}

pub type EmbeddingGeneratorTS = Arc<dyn EmbeddingGenerator + Sync + Send>;

/// A trait that defines the interface for generating text embeddings.
#[async_trait]
pub trait EmbeddingGenerator {
    /// Generates text embeddings for the given texts using the specified model.
    ///
    /// # Arguments
    ///
    /// * `inputs` - A vector of strings for which embeddings are to be generated.
    /// * `model` - The name of the model to be used for generating embeddings.
    ///
    /// # Returns
    ///
    /// * A result containing a vector of embeddings if successful, or an `EmbeddingGeneratorError`
    ///   if an error occurs.
    async fn generate_embeddings(
        &self,
        inputs: Vec<String>,
        model: String,
    ) -> Result<Vec<Vec<f32>>, EmbeddingGeneratorError>;

    /// Returns the number of dimensions of the embeddings generated by the specified model.
    ///
    /// # Arguments
    ///
    /// * `model` - The name of the model for which the number of dimensions is to be retrieved.
    ///
    /// # Returns
    ///
    /// * A result containing the number of dimensions if successful, or an `EmbeddingGeneratorError`
    ///   if an error occurs (e.g., the model is not found).
    fn dimensions(&self, model: String) -> Result<u64, EmbeddingGeneratorError>;

    // Tokenizes a list of inputs using the specified model.
    // This is for splitters to use to split inputs while respecting token boundaries.
    async fn tokenize_text(
        &self,
        inputs: Vec<String>,
        model: String,
    ) -> Result<Vec<Vec<String>>, EmbeddingGeneratorError>;

    async fn tokenize_encode(
        &self,
        inputs: Vec<String>,
        model: String,
    ) -> Result<Vec<Vec<i64>>, EmbeddingGeneratorError>;

    async fn tokenize_decode(
        &self,
        inputs: Vec<Vec<i64>>,
        model: String,
    ) -> Result<Vec<String>, EmbeddingGeneratorError>;
}

/// A struct that represents a router for generating text embeddings using different models.
///
/// This struct provides methods for generating text embeddings using various models.
/// It maintains a mapping between model names and their corresponding `EmbeddingGenerator`
/// implementations, allowing it to route embedding generation requests to the appropriate model.
pub struct EmbeddingRouter {
    router: HashMap<String, EmbeddingGeneratorTS>,

    model_names: Vec<String>,
}

impl EmbeddingRouter {
    /// Creates a new instance of `EmbeddingRouter` with the specified server configuration.
    ///
    /// This method initializes the router with the available models and their corresponding
    /// `EmbeddingGenerator` implementations.
    ///
    /// # Arguments
    ///
    /// * `config` - The server configuration specifying the available models and their settings.
    ///
    /// # Returns
    ///
    /// * A result containing an instance of `EmbeddingRouter` if successful, or an
    ///   `EmbeddingGeneratorError` if an error occurs.
    pub fn new(config: Arc<server_config::ServerConfig>) -> Result<Self, EmbeddingGeneratorError> {
        let mut router: HashMap<String, EmbeddingGeneratorTS> = HashMap::new();
        let mut sentence_transformers: Vec<server_config::EmbeddingModel> = Vec::new();
        let mut model_names = Vec::new();
        for model in config.available_models.clone() {
            model_names.push(model.model_kind.to_string());
            info!(
                "loading embedding model: {:?}",
                model.model_kind.to_string()
            );
            match model.model_kind {
                AllMiniLmL12V2 | T5Base | AllMiniLmL6V2 => {
                    sentence_transformers.push(model.clone());
                }
                OpenAIAda02 => {
                    let openai_config = config.openai.clone().ok_or(
                        EmbeddingGeneratorError::ConfigurationError(
                            "openai".into(),
                            "openai_config".into(),
                        ),
                    )?;
                    let openai_ada02 = OpenAI::new(openai_config, model.clone())?;
                    router.insert(model.model_kind.to_string(), Arc::new(openai_ada02));
                }
                _ => {
                    return Err(EmbeddingGeneratorError::InternalError(format!(
                        "model kind `{}` not supported",
                        model.model_kind
                    )))
                }
            }
        }
        let sentence_transformer_router = Arc::new(SentenceTransformerModels::new(
            sentence_transformers.clone(),
        )?);
        for st in sentence_transformers {
            router.insert(
                st.model_kind.to_string(),
                sentence_transformer_router.clone(),
            );
        }
        Ok(Self {
            router,
            model_names,
        })
    }

    /// Returns a list of available model names.
    ///
    /// # Returns
    ///
    /// * A vector of strings representing the names of the available models.
    pub fn list_models(&self) -> Vec<String> {
        self.model_names.clone()
    }
}

#[async_trait]
impl EmbeddingGenerator for EmbeddingRouter {
    async fn generate_embeddings(
        &self,
        inputs: Vec<String>,
        model: String,
    ) -> Result<Vec<Vec<f32>>, EmbeddingGeneratorError> {
        let embedding_model = self
            .router
            .get(&model)
            .ok_or(EmbeddingGeneratorError::ModelNotFound(model.clone()))?;
        embedding_model.generate_embeddings(inputs, model).await
    }

    fn dimensions(&self, model: String) -> Result<u64, EmbeddingGeneratorError> {
        let embedding_model = self
            .router
            .get(&model)
            .ok_or(EmbeddingGeneratorError::ModelLoadingError(model.clone()))?;
        embedding_model.dimensions(model)
    }

    async fn tokenize_text(
        &self,
        inputs: Vec<String>,
        model: String,
    ) -> Result<Vec<Vec<String>>, EmbeddingGeneratorError> {
        let embedding_model = self
            .router
            .get(&model)
            .ok_or(EmbeddingGeneratorError::ModelNotFound(model.clone()))?;
        embedding_model.tokenize_text(inputs, model).await
    }

    async fn tokenize_encode(
        &self,
        inputs: Vec<String>,
        model: String,
    ) -> Result<Vec<Vec<i64>>, EmbeddingGeneratorError> {
        let embedding_model = self
            .router
            .get(&model)
            .ok_or(EmbeddingGeneratorError::ModelNotFound(model.clone()))?;
        embedding_model.tokenize_encode(inputs, model).await
    }

    async fn tokenize_decode(
        &self,
        inputs: Vec<Vec<i64>>,
        model: String,
    ) -> Result<Vec<String>, EmbeddingGeneratorError> {
        let embedding_model = self
            .router
            .get(&model)
            .ok_or(EmbeddingGeneratorError::ModelNotFound(model.clone()))?;
        embedding_model.tokenize_decode(inputs, model).await
    }
}
