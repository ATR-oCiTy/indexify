# Welcome to Indexify

Indexify is a Retrieval and Long Term Memory Service for Generative AI Applications.

Indexes are always kept up to date by running extraction models such as embedding, NER, etc. on new documents that are uploaded to the service. Indexify has a built in distributed extraction scheduler that allows indexing large amount of data for production use cases.

It also provides APIs for LLM applications to retrieve information from indexes. Agents can store long term memory and query them in real time to personalize co-pilot or chat based applications.

Besides indexes of embedding vectors for semantic search, Indexify can also build indexes that contain K/V or JSON documents extracted by NER models, intent understanding, etc.

## Why use Indexify
* **Knowledge Base for LLMs:** Real time retrieval of knowledge and context from private documents and structured data to improve accuracy of LLM models.
* **Memory Engine for Co-Pilot agents:** Store and retrieve long-term memory of agents in real-time, providing enhanced personalization and improved user experiences for co-pilot and chat based applications.
* **Distributed Extraction Engine For Scale:** Distributed extraction to scale indexing large amount of data without sacrificing retrieval performance.
* **Custom Extractors:** You can extend Indexify by writing a custom extractor for your use cases to extract specific information from data.

## Start Using Indexify
Dive into [Getting Started](getting_started.md) to learn how to use Indexify.
